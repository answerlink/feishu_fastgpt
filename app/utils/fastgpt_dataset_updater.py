import asyncio
from typing import Dict, List, Optional
from collections import defaultdict
from datetime import datetime
from app.core.logger import setup_logger
from app.services.fastgpt_service import FastGPTService

logger = setup_logger("fastgpt_dataset_updater")

class FastGPTDatasetUpdater:
    """FastGPTçŸ¥è¯†åº“æè¿°æ›´æ–°å·¥å…·
    
    ç”¨äºæ‰«æFastGPTä¸­çš„æ‰€æœ‰çŸ¥è¯†åº“ï¼ˆdatasetï¼‰ï¼Œå¹¶ä¸ºå®ƒä»¬ç”Ÿæˆå’Œæ›´æ–°æè¿°ï¼Œè§„åˆ™ä¸ºï¼š
    1. é€’å½’éå†æ‰€æœ‰å¯è®¿é—®çš„æ–‡ä»¶å¤¹ã€çŸ¥è¯†åº“ï¼ˆdatasetï¼‰
    2. æ ¹æ®æ¨¡å¼å‚æ•°å†³å®šæ˜¯å¦è·³è¿‡å·²æœ‰æè¿°çš„çŸ¥è¯†åº“æˆ–å…¨é‡è¦†ç›–æ›´æ–°
    3. ä½¿ç”¨LLMæ ¹æ®çŸ¥è¯†åº“ä¸­çš„æ–‡ä»¶åˆ—è¡¨ç”Ÿæˆæè¿°
    """
    
    def __init__(self, app_id: str, skip_existing: bool = True, dry_run: bool = False):
        """åˆå§‹åŒ–æè¿°æ›´æ–°å·¥å…·
        
        Args:
            app_id: åº”ç”¨IDï¼Œç”¨äºè·å–å¯¹åº”çš„FastGPTé…ç½®
            skip_existing: å¦‚æœå·²æœ‰æè¿°å°±è·³è¿‡ï¼ŒFalseè¡¨ç¤ºå…¨é‡è¦†ç›–æ›´æ–°
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼ï¼ŒTrueè¡¨ç¤ºåªåˆ†æä¸æ›´æ–°
        """
        self.app_id = app_id
        self.skip_existing = skip_existing
        self.dry_run = dry_run
        self.fastgpt_service = FastGPTService(app_id)
        self.update_stats = {
            "scanned_folders": 0,
            "scanned_datasets": 0,
            "existing_descriptions": 0,
            "skipped_datasets": 0,
            "updated_datasets": 0,
            "would_update_datasets": 0,  # dry_runæ¨¡å¼ä¸‹é¢„è®¡æ›´æ–°çš„æ•°é‡
            "failed_updates": 0,
            "errors": []
        }
    
    async def close(self):
        """å…³é—­FastGPTæœåŠ¡è¿æ¥"""
        if self.fastgpt_service:
            await self.fastgpt_service.close()
    
    async def update_dataset_descriptions(self) -> Dict:
        """æ›´æ–°çŸ¥è¯†åº“æè¿°
        
        Returns:
            Dict: æ›´æ–°ç»“æœç»Ÿè®¡
        """
        try:
            mode_text = "é¢„è§ˆæ¨¡å¼" if self.dry_run else "æ›´æ–°æ¨¡å¼"
            skip_text = "è·³è¿‡å·²æœ‰æè¿°" if self.skip_existing else "å…¨é‡è¦†ç›–æ›´æ–°"
            logger.info(f"å¼€å§‹{mode_text}FastGPTçŸ¥è¯†åº“æè¿° - app_id: {self.app_id}, ç­–ç•¥: {skip_text}")
            
            # æ£€æŸ¥æ‘˜è¦LLMé…ç½®
            app_config = self.fastgpt_service.app_config
            if not all([
                app_config.summary_llm_api_url,
                app_config.summary_llm_api_key,
                app_config.summary_llm_model
            ]):
                error_msg = "æ‘˜è¦LLMé…ç½®ä¸å®Œæ•´ï¼Œæ— æ³•ç”ŸæˆçŸ¥è¯†åº“æè¿°"
                logger.error(error_msg)
                return {
                    "code": 400,
                    "message": error_msg,
                    "data": self.update_stats
                }
            
            # ä»æ ¹ç›®å½•å¼€å§‹é€’å½’éå†
            await self._process_directory(parent_id=None)
            
            # æ‰“å°æ›´æ–°ç»Ÿè®¡
            self._log_update_summary()
            
            message = "é¢„è§ˆå®Œæˆ" if self.dry_run else "æ›´æ–°å®Œæˆ"
            return {
                "code": 200,
                "message": message,
                "data": self.update_stats
            }
            
        except Exception as e:
            error_msg = f"æ›´æ–°è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}"
            logger.error(error_msg)
            self.update_stats["errors"].append(error_msg)
            return {
                "code": 500,
                "message": error_msg,
                "data": self.update_stats
            }
        finally:
            await self.close()
    
    async def _process_directory(self, parent_id: Optional[str], level: int = 0) -> None:
        """é€’å½’å¤„ç†ç›®å½•
        
        Args:
            parent_id: çˆ¶ç›®å½•IDï¼ŒNoneè¡¨ç¤ºæ ¹ç›®å½•
            level: é€’å½’å±‚çº§ï¼Œç”¨äºæ—¥å¿—ç¼©è¿›
        """
        indent = "  " * level
        logger.info(f"{indent}å¼€å§‹å¤„ç†ç›®å½• - parent_id: {parent_id or 'ROOT'}")
        
        try:
            # è·å–å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰é¡¹ç›®
            result = await self.fastgpt_service.get_dataset_list(parent_id)
            
            if result.get("code") != 200:
                error_msg = f"{indent}è·å–ç›®å½•åˆ—è¡¨å¤±è´¥: {result.get('message')}"
                logger.error(error_msg)
                self.update_stats["errors"].append(error_msg)
                return
            
            items = result.get("data", [])
            if not items:
                logger.info(f"{indent}ç›®å½•ä¸ºç©º")
                return
            
            logger.info(f"{indent}æ‰¾åˆ° {len(items)} ä¸ªé¡¹ç›®")
            
            # åˆ†ç±»é¡¹ç›®
            folders = []
            datasets = []
            
            for item in items:
                item_type = item.get("type", "")
                item_name = item.get("name", "")
                item_id = item.get("_id", "")
                
                if item_type == "folder":
                    folders.append(item)
                    logger.info(f"{indent}å‘ç°æ–‡ä»¶å¤¹: {item_name} (ID: {item_id})")
                elif item_type == "dataset":
                    datasets.append(item)
                    logger.info(f"{indent}å‘ç°çŸ¥è¯†åº“: {item_name} (ID: {item_id})")
                else:
                    logger.warning(f"{indent}å‘ç°æœªçŸ¥ç±»å‹é¡¹ç›®: {item_name} (ç±»å‹: {item_type})")
            
            # æ›´æ–°ç»Ÿè®¡
            self.update_stats["scanned_folders"] += len(folders)
            self.update_stats["scanned_datasets"] += len(datasets)
            
            # å¤„ç†æ‰€æœ‰çŸ¥è¯†åº“çš„æè¿°
            for dataset in datasets:
                await self._process_dataset(dataset, level + 1)
            
            # é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹
            for folder in folders:
                await self._process_directory(folder.get("_id"), level + 1)
                
        except Exception as e:
            error_msg = f"{indent}å¤„ç†ç›®å½•å¼‚å¸¸: {str(e)}"
            logger.error(error_msg)
            self.update_stats["errors"].append(error_msg)
    
    async def _process_dataset(self, dataset: Dict, level: int) -> None:
        """å¤„ç†å•ä¸ªçŸ¥è¯†åº“
        
        Args:
            dataset: çŸ¥è¯†åº“ä¿¡æ¯
            level: é€’å½’å±‚çº§ï¼Œç”¨äºæ—¥å¿—ç¼©è¿›
        """
        indent = "  " * level
        dataset_id = dataset.get("_id", "")
        dataset_name = dataset.get("name", "")
        current_intro = dataset.get("intro", "")
        
        logger.info(f"{indent}å¼€å§‹å¤„ç†çŸ¥è¯†åº“: {dataset_name} (ID: {dataset_id})")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æè¿°
        has_description = bool(current_intro and current_intro.strip())
        if has_description:
            self.update_stats["existing_descriptions"] += 1
            logger.info(f"{indent}å½“å‰æè¿°: {current_intro}")
        
        # æ ¹æ®è·³è¿‡ç­–ç•¥å†³å®šæ˜¯å¦å¤„ç†
        if has_description and self.skip_existing:
            self.update_stats["skipped_datasets"] += 1
            logger.info(f"{indent}å·²æœ‰æè¿°ä¸”é…ç½®ä¸ºè·³è¿‡ï¼Œè·³è¿‡çŸ¥è¯†åº“: {dataset_name}")
            return
        
        try:
            # è·å–çŸ¥è¯†åº“ä¸­çš„collections
            collections_result = await self.fastgpt_service.get_collection_list(dataset_id)
            
            if collections_result.get("code") != 200:
                error_msg = f"{indent}è·å–çŸ¥è¯†åº“collectionså¤±è´¥: {collections_result.get('message')}"
                logger.error(error_msg)
                self.update_stats["errors"].append(error_msg)
                return
            
            collections = collections_result.get("data", {}).get("list", [])
            
            if not collections:
                logger.info(f"{indent}çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡ä»¶ï¼Œè·³è¿‡æè¿°ç”Ÿæˆ")
                return
            
            logger.info(f"{indent}çŸ¥è¯†åº“ä¸­å…±æœ‰ {len(collections)} ä¸ªæ–‡ä»¶")
            
            # æå–æ–‡ä»¶å
            filenames = [collection.get("name", "") for collection in collections if collection.get("name")]
            
            if not filenames:
                logger.info(f"{indent}æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶åï¼Œè·³è¿‡æè¿°ç”Ÿæˆ")
                return
            
            # ç”Ÿæˆæ–°æè¿°
            await self._generate_and_update_description(dataset_id, dataset_name, filenames, level)
                
        except Exception as e:
            error_msg = f"{indent}å¤„ç†çŸ¥è¯†åº“å¼‚å¸¸: {dataset_name}, é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            self.update_stats["errors"].append(error_msg)
    
    async def _generate_and_update_description(self, dataset_id: str, dataset_name: str, filenames: List[str], level: int) -> None:
        """ç”Ÿæˆå¹¶æ›´æ–°çŸ¥è¯†åº“æè¿°
        
        Args:
            dataset_id: çŸ¥è¯†åº“ID
            dataset_name: çŸ¥è¯†åº“åç§°
            filenames: æ–‡ä»¶ååˆ—è¡¨
            level: é€’å½’å±‚çº§ï¼Œç”¨äºæ—¥å¿—ç¼©è¿›
        """
        indent = "  " * level
        
        if self.dry_run:
            # é¢„è§ˆæ¨¡å¼ï¼Œåªè®°å½•ä¸æ›´æ–°
            self.update_stats["would_update_datasets"] += 1
            file_list = ", ".join(filenames[:5])  # åªæ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶å
            if len(filenames) > 5:
                file_list += f"... ç­‰{len(filenames)}ä¸ªæ–‡ä»¶"
            logger.info(f"{indent}ğŸ” [é¢„è§ˆ] å°†ä¸ºçŸ¥è¯†åº“ç”Ÿæˆæè¿°: '{dataset_name}' (ID: {dataset_id}, æ–‡ä»¶: {file_list})")
            return
        
        logger.info(f"{indent}å¼€å§‹ä¸ºçŸ¥è¯†åº“ç”Ÿæˆæè¿°: {dataset_name} (æ–‡ä»¶æ•°é‡: {len(filenames)})")
        
        try:
            # è°ƒç”¨æ‘˜è¦LLMç”Ÿæˆæè¿°
            app_config = self.fastgpt_service.app_config
            description = await self.fastgpt_service.call_summary_llm(
                "è¯·ç›´æ¥ç»™è¿™ä¸ªæ–‡ä»¶å¤¹æ·»åŠ ä¸€æ®µç®€æ´æ˜“æ‡‚çš„æè¿°ï¼Œè®©ç”¨æˆ·å¯ä»¥å¿«é€Ÿäº†è§£è¿™ä¸ªæ–‡ä»¶å¤¹çš„å†…å®¹ã€‚ä¸è¦åšé¢å¤–è§£é‡Šè¯´æ˜ã€‚",
                filenames
            )
            
            if not description:
                logger.warning(f"{indent}LLMæœªç”Ÿæˆæœ‰æ•ˆæè¿°: {dataset_name}")
                self.update_stats["failed_updates"] += 1
                return
            
            # æ›´æ–°çŸ¥è¯†åº“æè¿°
            result = await self.fastgpt_service.update_dataset_description(dataset_id, description)
            
            if result.get("code") == 200:
                self.update_stats["updated_datasets"] += 1
                logger.info(f"{indent}âœ“ æˆåŠŸæ›´æ–°çŸ¥è¯†åº“æè¿°: '{dataset_name}' -> '{description}'")
            else:
                self.update_stats["failed_updates"] += 1
                error_msg = f"{indent}âœ— æ›´æ–°çŸ¥è¯†åº“æè¿°å¤±è´¥: '{dataset_name}', é”™è¯¯: {result.get('message')}"
                logger.error(error_msg)
                self.update_stats["errors"].append(error_msg)
                
        except Exception as e:
            self.update_stats["failed_updates"] += 1
            error_msg = f"{indent}âœ— ç”Ÿæˆæè¿°å¼‚å¸¸: '{dataset_name}', é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            self.update_stats["errors"].append(error_msg)
    
    def _log_update_summary(self) -> None:
        """æ‰“å°æ›´æ–°ç»Ÿè®¡æ‘˜è¦"""
        stats = self.update_stats
        mode_text = "é¢„è§ˆ" if self.dry_run else "æ›´æ–°"
        skip_text = "è·³è¿‡å·²æœ‰æè¿°" if self.skip_existing else "å…¨é‡è¦†ç›–æ›´æ–°"
        
        logger.info("=" * 60)
        logger.info(f"FastGPTçŸ¥è¯†åº“æè¿°{mode_text}ç»Ÿè®¡æ‘˜è¦ ({skip_text})")
        logger.info("=" * 60)
        logger.info(f"æ‰«æçš„æ–‡ä»¶å¤¹æ•°é‡: {stats['scanned_folders']}")
        logger.info(f"æ‰«æçš„çŸ¥è¯†åº“æ•°é‡: {stats['scanned_datasets']}")
        logger.info(f"å·²æœ‰æè¿°çš„çŸ¥è¯†åº“æ•°é‡: {stats['existing_descriptions']}")
        logger.info(f"è·³è¿‡çš„çŸ¥è¯†åº“æ•°é‡: {stats['skipped_datasets']}")
        
        if self.dry_run:
            logger.info(f"é¢„è®¡æ›´æ–°çš„çŸ¥è¯†åº“æ•°é‡: {stats['would_update_datasets']}")
        else:
            logger.info(f"æˆåŠŸæ›´æ–°çš„çŸ¥è¯†åº“æ•°é‡: {stats['updated_datasets']}")
            logger.info(f"æ›´æ–°å¤±è´¥çš„çŸ¥è¯†åº“æ•°é‡: {stats['failed_updates']}")
        
        logger.info(f"é”™è¯¯æ•°é‡: {len(stats['errors'])}")
        
        if stats['errors']:
            logger.warning("é”™è¯¯è¯¦æƒ…:")
            for i, error in enumerate(stats['errors'], 1):
                logger.warning(f"  {i}. {error}")
        
        logger.info("=" * 60) 