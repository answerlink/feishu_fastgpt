import asyncio
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
from datetime import datetime
from app.core.logger import setup_logger
from app.services.fastgpt_service import FastGPTService

logger = setup_logger("fastgpt_cleaner")

class FastGPTCleaner:
    """FastGPTçŸ¥è¯†åº“æ¸…ç†å·¥å…·
    
    ç”¨äºæ¸…ç†FastGPTä¸­é‡å¤çš„collectionï¼ˆçŸ¥è¯†æ–‡ä»¶ï¼‰ï¼Œè§„åˆ™ä¸ºï¼š
    1. é€’å½’éå†æ‰€æœ‰å¯è®¿é—®çš„æ–‡ä»¶å¤¹ã€çŸ¥è¯†åº“ï¼ˆdatasetï¼‰ã€æ–‡ä»¶ï¼ˆcollectionï¼‰
    2. åœ¨ä¸€ä¸ªçŸ¥è¯†åº“å†…å‘ç°collectionæœ‰é‡åæ—¶ï¼Œä¿ç•™æœ€æ–°æ—¶é—´çš„ï¼Œåˆ é™¤å…¶ä»–çš„
    """
    
    def __init__(self, app_id: str, dry_run: bool = False):
        """åˆå§‹åŒ–æ¸…ç†å·¥å…·
        
        Args:
            app_id: åº”ç”¨IDï¼Œç”¨äºè·å–å¯¹åº”çš„FastGPTé…ç½®
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼ï¼ŒTrueè¡¨ç¤ºåªåˆ†æä¸åˆ é™¤
        """
        self.app_id = app_id
        self.dry_run = dry_run
        self.fastgpt_service = FastGPTService(app_id)
        self.cleanup_stats = {
            "scanned_folders": 0,
            "scanned_datasets": 0,
            "scanned_collections": 0,
            "found_duplicates": 0,
            "deleted_collections": 0,
            "would_delete_collections": 0,  # dry_runæ¨¡å¼ä¸‹é¢„è®¡åˆ é™¤çš„æ•°é‡
            "errors": []
        }
    
    async def close(self):
        """å…³é—­FastGPTæœåŠ¡è¿æ¥"""
        if self.fastgpt_service:
            await self.fastgpt_service.close()
    
    async def clean_duplicate_collections(self) -> Dict:
        """æ¸…ç†é‡å¤çš„collections
        
        Returns:
            Dict: æ¸…ç†ç»“æœç»Ÿè®¡
        """
        try:
            mode_text = "é¢„è§ˆæ¨¡å¼" if self.dry_run else "æ¸…ç†æ¨¡å¼"
            logger.info(f"å¼€å§‹{mode_text}FastGPTé‡å¤collections - app_id: {self.app_id}")
            
            # ä»æ ¹ç›®å½•å¼€å§‹é€’å½’éå†
            await self._process_directory(parent_id=None)
            
            # æ‰“å°æ¸…ç†ç»Ÿè®¡
            self._log_cleanup_summary()
            
            message = "é¢„è§ˆå®Œæˆ" if self.dry_run else "æ¸…ç†å®Œæˆ"
            return {
                "code": 200,
                "message": message,
                "data": self.cleanup_stats
            }
            
        except Exception as e:
            error_msg = f"æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}"
            logger.error(error_msg)
            self.cleanup_stats["errors"].append(error_msg)
            return {
                "code": 500,
                "message": error_msg,
                "data": self.cleanup_stats
            }
        finally:
            await self.close()
    
    async def _process_directory(self, parent_id: Optional[str], level: int = 0) -> None:
        """é€’å½’å¤„ç†ç›®å½•
        
        Args:
            parent_id: çˆ¶ç›®å½•IDï¼ŒNoneè¡¨ç¤ºæ ¹ç›®å½•
            level: é€’å½’å±‚çº§ï¼Œç”¨äºæ—¥å¿—ç¼©è¿›
        """
        indent = "  " * level
        logger.info(f"{indent}å¼€å§‹å¤„ç†ç›®å½• - parent_id: {parent_id or 'ROOT'}")
        
        try:
            # è·å–å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰é¡¹ç›®
            result = await self.fastgpt_service.get_dataset_list(parent_id)
            
            if result.get("code") != 200:
                error_msg = f"{indent}è·å–ç›®å½•åˆ—è¡¨å¤±è´¥: {result.get('message')}"
                logger.error(error_msg)
                self.cleanup_stats["errors"].append(error_msg)
                return
            
            items = result.get("data", [])
            if not items:
                logger.info(f"{indent}ç›®å½•ä¸ºç©º")
                return
            
            logger.info(f"{indent}æ‰¾åˆ° {len(items)} ä¸ªé¡¹ç›®")
            
            # åˆ†ç±»é¡¹ç›®
            folders = []
            datasets = []
            
            for item in items:
                item_type = item.get("type", "")
                item_name = item.get("name", "")
                item_id = item.get("_id", "")
                
                if item_type == "folder":
                    folders.append(item)
                    logger.info(f"{indent}å‘ç°æ–‡ä»¶å¤¹: {item_name} (ID: {item_id})")
                elif item_type == "dataset":
                    datasets.append(item)
                    logger.info(f"{indent}å‘ç°çŸ¥è¯†åº“: {item_name} (ID: {item_id})")
                else:
                    logger.warning(f"{indent}å‘ç°æœªçŸ¥ç±»å‹é¡¹ç›®: {item_name} (ç±»å‹: {item_type})")
            
            # æ›´æ–°ç»Ÿè®¡
            self.cleanup_stats["scanned_folders"] += len(folders)
            self.cleanup_stats["scanned_datasets"] += len(datasets)
            
            # å¤„ç†æ‰€æœ‰çŸ¥è¯†åº“ä¸­çš„collections
            for dataset in datasets:
                await self._process_dataset(dataset, level + 1)
            
            # é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹
            for folder in folders:
                await self._process_directory(folder.get("_id"), level + 1)
                
        except Exception as e:
            error_msg = f"{indent}å¤„ç†ç›®å½•å¼‚å¸¸: {str(e)}"
            logger.error(error_msg)
            self.cleanup_stats["errors"].append(error_msg)
    
    async def _process_dataset(self, dataset: Dict, level: int) -> None:
        """å¤„ç†å•ä¸ªçŸ¥è¯†åº“
        
        Args:
            dataset: çŸ¥è¯†åº“ä¿¡æ¯
            level: é€’å½’å±‚çº§ï¼Œç”¨äºæ—¥å¿—ç¼©è¿›
        """
        indent = "  " * level
        dataset_id = dataset.get("_id", "")
        dataset_name = dataset.get("name", "")
        
        logger.info(f"{indent}å¼€å§‹å¤„ç†çŸ¥è¯†åº“: {dataset_name} (ID: {dataset_id})")
        
        try:
            # è·å–çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰collectionsï¼Œåˆ†é¡µè·å–æ‰€æœ‰æ•°æ®
            all_collections = await self._get_all_collections(dataset_id)
            
            if not all_collections:
                logger.info(f"{indent}çŸ¥è¯†åº“ä¸­æ²¡æœ‰collections")
                return
            
            self.cleanup_stats["scanned_collections"] += len(all_collections)
            logger.info(f"{indent}çŸ¥è¯†åº“ä¸­å…±æœ‰ {len(all_collections)} ä¸ªcollections")
            
            # æŒ‰åç§°åˆ†ç»„
            collections_by_name = defaultdict(list)
            for collection in all_collections:
                collection_name = collection.get("name", "")
                if collection_name:
                    collections_by_name[collection_name].append(collection)
            
            # æŸ¥æ‰¾é‡å¤é¡¹
            duplicates_found = 0
            for name, collections in collections_by_name.items():
                if len(collections) > 1:
                    duplicates_found += 1
                    await self._handle_duplicate_collections(name, collections, level + 1)
            
            if duplicates_found > 0:
                self.cleanup_stats["found_duplicates"] += duplicates_found
                logger.info(f"{indent}çŸ¥è¯†åº“å¤„ç†å®Œæˆï¼Œå‘ç° {duplicates_found} ç»„é‡å¤collections")
            else:
                logger.info(f"{indent}çŸ¥è¯†åº“å¤„ç†å®Œæˆï¼Œæœªå‘ç°é‡å¤collections")
                
        except Exception as e:
            error_msg = f"{indent}å¤„ç†çŸ¥è¯†åº“å¼‚å¸¸: {dataset_name}, é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            self.cleanup_stats["errors"].append(error_msg)
    
    async def _get_all_collections(self, dataset_id: str) -> List[Dict]:
        """è·å–çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰collectionsï¼ˆåˆ†é¡µè·å–ï¼‰
        
        Args:
            dataset_id: çŸ¥è¯†åº“ID
            
        Returns:
            List[Dict]: æ‰€æœ‰collectionsåˆ—è¡¨
        """
        all_collections = []
        offset = 0
        page_size = 30  # FastGPTé»˜è®¤æœ€å¤§30
        
        while True:
            # ä¿®æ”¹è¯·æ±‚æ•°æ®ç»“æ„ä»¥æ”¯æŒåˆ†é¡µ
            data = {
                "offset": offset,
                "pageSize": page_size,
                "datasetId": dataset_id,
                "parentId": "",
                "searchText": ""
            }
            
            result = await self.fastgpt_service._request("POST", "/api/core/dataset/collection/listV2", data)
            
            if result.get("code") != 200:
                logger.error(f"è·å–collectionsåˆ—è¡¨å¤±è´¥: {result.get('message')}")
                break
            
            collections_data = result.get("data", {})
            collections = collections_data.get("list", [])
            total = collections_data.get("total", 0)
            
            if not collections:
                break
            
            all_collections.extend(collections)
            offset += len(collections)
            
            logger.info(f"å·²è·å– {len(all_collections)}/{total} ä¸ªcollections")
            
            # å¦‚æœè·å–çš„æ•°é‡å°‘äºé¡µé¢å¤§å°ï¼Œè¯´æ˜å·²ç»è·å–å®Œæ¯•
            if len(collections) < page_size:
                break
        
        return all_collections
    
    async def _handle_duplicate_collections(self, name: str, collections: List[Dict], level: int) -> None:
        """å¤„ç†é‡å¤çš„collections
        
        Args:
            name: collectionåç§°
            collections: é‡å¤çš„collectionsåˆ—è¡¨
            level: é€’å½’å±‚çº§ï¼Œç”¨äºæ—¥å¿—ç¼©è¿›
        """
        indent = "  " * level
        logger.warning(f"{indent}å‘ç°é‡å¤collections: '{name}' (å…± {len(collections)} ä¸ª)")
        
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„
        sorted_collections = sorted(
            collections,
            key=lambda x: self._parse_time(x.get("createTime", "")),
            reverse=True
        )
        
        # ä¿ç•™æœ€æ–°çš„
        keep_collection = sorted_collections[0]
        delete_collections = sorted_collections[1:]
        
        keep_time = keep_collection.get("createTime", "")
        keep_id = keep_collection.get("_id", "")
        
        logger.info(f"{indent}ä¿ç•™æœ€æ–°çš„collection: '{name}' (ID: {keep_id}, åˆ›å»ºæ—¶é—´: {keep_time})")
        
        # åˆ é™¤å…¶ä»–çš„
        for collection in delete_collections:
            await self._delete_collection_safe(collection, level)
    
    def _parse_time(self, time_str: str) -> datetime:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²
        
        Args:
            time_str: æ—¶é—´å­—ç¬¦ä¸²
            
        Returns:
            datetime: è§£æåçš„æ—¶é—´å¯¹è±¡
        """
        try:
            # å°è¯•è§£æISOæ ¼å¼æ—¶é—´
            if time_str:
                # ç§»é™¤æ—¶åŒºä¿¡æ¯åè§£æ
                if 'T' in time_str:
                    time_part = time_str.split('+')[0].split('Z')[0]
                    return datetime.fromisoformat(time_part)
                else:
                    return datetime.fromisoformat(time_str)
        except Exception as e:
            logger.warning(f"è§£ææ—¶é—´å¤±è´¥: {time_str}, é”™è¯¯: {str(e)}")
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›æœ€å°æ—¶é—´
        return datetime.min
    
    async def _delete_collection_safe(self, collection: Dict, level: int) -> None:
        """å®‰å…¨åˆ é™¤collection
        
        Args:
            collection: è¦åˆ é™¤çš„collectionä¿¡æ¯
            level: é€’å½’å±‚çº§ï¼Œç”¨äºæ—¥å¿—ç¼©è¿›
        """
        indent = "  " * level
        collection_id = collection.get("_id", "")
        collection_name = collection.get("name", "")
        create_time = collection.get("createTime", "")
        
        if self.dry_run:
            # é¢„è§ˆæ¨¡å¼ï¼Œåªè®°å½•ä¸åˆ é™¤
            self.cleanup_stats["would_delete_collections"] += 1
            logger.info(f"{indent}ğŸ” [é¢„è§ˆ] å°†åˆ é™¤é‡å¤collection: '{collection_name}' (ID: {collection_id}, åˆ›å»ºæ—¶é—´: {create_time})")
            return
        
        logger.info(f"{indent}å‡†å¤‡åˆ é™¤é‡å¤collection: '{collection_name}' (ID: {collection_id}, åˆ›å»ºæ—¶é—´: {create_time})")
        
        try:
            result = await self.fastgpt_service.delete_collection(collection_id)
            
            if result.get("code") == 200:
                self.cleanup_stats["deleted_collections"] += 1
                logger.info(f"{indent}âœ“ æˆåŠŸåˆ é™¤é‡å¤collection: '{collection_name}' (ID: {collection_id})")
            else:
                error_msg = f"{indent}âœ— åˆ é™¤collectionå¤±è´¥: '{collection_name}', é”™è¯¯: {result.get('msg')}"
                logger.error(error_msg)
                self.cleanup_stats["errors"].append(error_msg)
                
        except Exception as e:
            error_msg = f"{indent}âœ— åˆ é™¤collectionå¼‚å¸¸: '{collection_name}', é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            self.cleanup_stats["errors"].append(error_msg)
    
    def _log_cleanup_summary(self) -> None:
        """æ‰“å°æ¸…ç†ç»Ÿè®¡æ‘˜è¦"""
        stats = self.cleanup_stats
        mode_text = "é¢„è§ˆ" if self.dry_run else "æ¸…ç†"
        
        logger.info("=" * 60)
        logger.info(f"FastGPT {mode_text}ç»Ÿè®¡æ‘˜è¦")
        logger.info("=" * 60)
        logger.info(f"æ‰«æçš„æ–‡ä»¶å¤¹æ•°é‡: {stats['scanned_folders']}")
        logger.info(f"æ‰«æçš„çŸ¥è¯†åº“æ•°é‡: {stats['scanned_datasets']}")
        logger.info(f"æ‰«æçš„collectionæ•°é‡: {stats['scanned_collections']}")
        logger.info(f"å‘ç°é‡å¤ç»„æ•°: {stats['found_duplicates']}")
        
        if self.dry_run:
            logger.info(f"é¢„è®¡åˆ é™¤çš„collectionæ•°é‡: {stats['would_delete_collections']}")
        else:
            logger.info(f"å®é™…åˆ é™¤çš„collectionæ•°é‡: {stats['deleted_collections']}")
        
        logger.info(f"é”™è¯¯æ•°é‡: {len(stats['errors'])}")
        
        if stats['errors']:
            logger.warning("é”™è¯¯è¯¦æƒ…:")
            for i, error in enumerate(stats['errors'], 1):
                logger.warning(f"  {i}. {error}")
        
        logger.info("=" * 60) 